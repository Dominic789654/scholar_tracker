# Citation Statistics

Last updated: 2026-02-16

## Overall Statistics
- Total Citations: 886
- H-index: 12

## Today's Citation Changes 

Total increase: +24 citations

| Paper | Previous | New | Increase |
| ----- | --------- | --- | -------- |
| LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning | 137 | 140 | +3 |
| Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models | 82 | 83 | +1 |
| Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models | 42 | 44 | +2 |
| LongGenBench: Long-context Generation Benchmark | 39 | 43 | +4 |
| Plum: Prompt Learning Using Metaheuristics | 38 | 42 | +4 |
| ChunkKV: Semantic-preserving kv cache compression for efficient long-context llm inference | 17 | 20 | +3 |
| Can LLMs Maintain Fundamental Abilities under KV Cache Compression? | 12 | 13 | +1 |
| Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing | 11 | 12 | +1 |
| Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression | 9 | 10 | +1 |
| The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve? | 6 | 9 | +3 |
| OracleKV: Oracle Guidance for Question-Independent KV Cache Compression | 4 | 6 | +2 |
| Antkv: Anchor token-aware sub-bit vector quantization for kv cache in large language models | 0 | 3 | +3 |
| Ssr: Speculative parallel scaling reasoning in test-time | 0 | 2 | +2 |
| FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management | 1 | 2 | +1 |
| Cafes: A collaborative multi-agent framework for multi-granular multimodal essay scoring | 0 | 2 | +2 |

## Paper Citations

| Paper | Citations | Year |
| ----- | --------- | ---- |
| Active prompting with chain-of-thought for large language models | 360 | 2023 |
| LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning | 140 | 2024 |
| Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models | 83 | 2024 |
| Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models | 44 | 2024 |
| LongGenBench: Long-context Generation Benchmark | 43 | 2024 |
| Plum: Prompt Learning Using Metaheuristics | 42 | 2024 |
| ParZC: Parametric Zero-Cost Proxies for Efficient NAS | 22 | 2024 |
| ChunkKV: Semantic-preserving kv cache compression for efficient long-context llm inference | 20 | 2025 |
| Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models | 20 | 2023 |
| Can LLMs Maintain Fundamental Abilities under KV Cache Compression? | 13 | 2025 |
| Should We Really Edit Language Models? On the Evaluation of Edited Language Models | 13 | 2024 |
| 3D Question Answering for City Scene Understanding | 13 | 2024 |
| Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing | 12 | 2025 |
| EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models | 11 | 2025 |
| Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression | 10 | 2025 |
| The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve? | 9 | 2025 |
| LPZero: Language Model Zero-cost Proxy Search from Zero | 8 | 2024 |
| OracleKV: Oracle Guidance for Question-Independent KV Cache Compression | 6 | 2025 |
| Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research | 4 | 2025 |
| Antkv: Anchor token-aware sub-bit vector quantization for kv cache in large language models | 3 | 2025 |
| Ssr: Speculative parallel scaling reasoning in test-time | 2 | 2025 |
| FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management | 2 | 2025 |
| Cafes: A collaborative multi-agent framework for multi-granular multimodal essay scoring | 2 | 2025 |
| DiffAdapt: Difficulty-Adaptive Reasoning for Token-Efficient LLM Inference | 1 | 2026 |
| Reasoning Language Model Inference Serving Unveiled: An Empirical Study | 1 | 2026 |
| Beyond Single Embeddings: Capturing Diverse Targets with Multi-Query Retrieval | 1 | 2025 |
| City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning | 1 | 2025 |
| From Literature to Lab: Closed-Loop Advancement of Perovskite Solar Cells via Domain Knowledge Guided LLM | 0 | 2026 |
| SONIC: Segmented Optimized Nexus for Information Compression in Key-Value Caching | 0 | 2026 |
| An Improved Autoregressive Evaluation Paradigm for Large Language Models | 0 | 2025 |

## Citation History

| Date | Total Citations | H-index |
| ---- | --------------- | ------- |
| 2026-02-16 | 886 | 12 |
| 2026-02-15 | 862 | 12 |
| 2026-02-14 | 862 | 12 |
| 2026-02-03 | 836 | 0 |
| 2026-01-31 | 833 | 0 |
| 2026-01-30 | 830 | 0 |
| 2026-01-25 | 810 | 0 |
| 2026-01-19 | 789 | 0 |
| 2026-01-09 | 778 | 11 |
| 2026-01-07 | 777 | 11 |

## Citation Trends

### Overall Trends
![Citation Trends](citation_trends.png)

### Individual Paper Trends
![Paper Trends](paper_trends.png)

*For interactive charts, see [citation_trends.html](citation_trends.html) and [paper_trends.html](paper_trends.html)*