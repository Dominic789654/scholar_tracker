# Citation Statistics

Last updated: 2025-03-12

## Overall Statistics
- Total Citations: 317
- H-index: 6

## Today's Citation Changes 

Total increase: +7 citations

| Paper | Previous | New | Increase |
| ----- | --------- | --- | -------- |
| Active prompting with chain-of-thought for large language models | 200 | 202 | +2 |
| LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning | 40 | 41 | +1 |
| Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models | 32 | 33 | +1 |
| ParZC: Parametric Zero-Cost Proxies for Efficient NAS | 5 | 6 | +1 |
| Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models | 4 | 6 | +2 |

## Paper Citations

| Paper | Citations | Year |
| ----- | --------- | ---- |
| Active prompting with chain-of-thought for large language models | 202 | 2023 |
| LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning | 41 | 2024 |
| Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models | 33 | 2024 |
| Plum: Prompt learning using metaheuristic | 13 | 2023 |
| Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models | 7 | 2023 |
| ParZC: Parametric Zero-Cost Proxies for Efficient NAS | 6 | 2024 |
| Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models | 6 | 2024 |
| LongGenBench: Long-context Generation Benchmark | 4 | 2024 |
| Should We Really Edit Language Models? On the Evaluation of Edited Language Models | 3 | 2024 |
| LPZero: Language Model Zero-cost Proxy Search from Zero | 1 | 2024 |
| 3D Question Answering for City Scene Understanding | 1 | 2024 |
| The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve? | 0 | 2025 |
| EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models | 0 | 2025 |
| Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing | 0 | 2025 |
| Can LLMs Maintain Fundamental Abilities under KV Cache Compression? | 0 | 2025 |
| ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference | 0 | 2025 |

## Citation History

| Date | Total Citations | H-index |
| ---- | --------------- | ------- |
| 2025-03-12 | 317 | 6 |
| 2025-03-10 | 310 | 5 |
| 2025-03-09 | 310 | 5 |
| 2025-03-08 | 310 | 5 |
| 2025-03-07 | 309 | 5 |
| 2025-03-06 | 308 | 5 |
| 2025-03-05 | 308 | 5 |
| 2025-03-04 | 304 | 5 |
| 2025-03-03 | 303 | 5 |
| 2025-03-02 | 300 | 5 |

## Citation Trends

### Overall Trends
![Citation Trends](citation_trends.png)

### Individual Paper Trends
![Paper Trends](paper_trends.png)

*For interactive charts, see [citation_trends.html](citation_trends.html) and [paper_trends.html](paper_trends.html)*