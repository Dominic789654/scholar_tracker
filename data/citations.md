# Citation Statistics

Last updated: 2025-07-21

## Overall Statistics
- Total Citations: 504
- H-index: 8

## Today's Citation Changes 

Total increase: +137 citations

| Paper | Previous | New | Increase |
| ----- | --------- | --- | -------- |
| Active prompting with chain-of-thought for large language models | 219 | 255 | +36 |
| LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning | 46 | 76 | +30 |
| Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models | 41 | 55 | +14 |
| LongGenBench: Long-context Generation Benchmark | 7 | 21 | +14 |
| Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models | 10 | 17 | +7 |
| Plum: Prompt learning using metaheuristic | 15 | 17 | +2 |
| Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models | 8 | 13 | +5 |
| ParZC: Parametric Zero-Cost Proxies for Efficient NAS | 7 | 11 | +4 |
| Should We Really Edit Language Models? On the Evaluation of Edited Language Models | 3 | 7 | +4 |
| EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models | 1 | 5 | +4 |
| Can LLMs Maintain Fundamental Abilities under KV Cache Compression? | 2 | 5 | +3 |
| 3D Question Answering for City Scene Understanding | 1 | 5 | +4 |
| The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve? | 1 | 4 | +3 |
| Chunkkv: Semantic-preserving kv cache compression for efficient long-context llm inference | 0 | 4 | +4 |
| Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression | 0 | 3 | +3 |
| Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing | 0 | 2 | +2 |
| OracleKV: Oracle Guidance for Question-Independent KV Cache Compression | 0 | 1 | +1 |

## Paper Citations

| Paper | Citations | Year |
| ----- | --------- | ---- |
| Active prompting with chain-of-thought for large language models | 255 | 2023 |
| LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning | 76 | 2024 |
| Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models | 55 | 2024 |
| LongGenBench: Long-context Generation Benchmark | 21 | 2024 |
| Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models | 17 | 2024 |
| Plum: Prompt learning using metaheuristic | 17 | 2023 |
| Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models | 13 | 2023 |
| ParZC: Parametric Zero-Cost Proxies for Efficient NAS | 11 | 2024 |
| Should We Really Edit Language Models? On the Evaluation of Edited Language Models | 7 | 2024 |
| EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models | 5 | 2025 |
| Can LLMs Maintain Fundamental Abilities under KV Cache Compression? | 5 | 2025 |
| 3D Question Answering for City Scene Understanding | 5 | 2024 |
| The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve? | 4 | 2025 |
| Chunkkv: Semantic-preserving kv cache compression for efficient long-context llm inference | 4 | 2025 |
| Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression | 3 | 2025 |
| Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing | 2 | 2025 |
| LPZero: Language Model Zero-cost Proxy Search from Zero | 2 | 2024 |
| Perovskite-llm: Knowledge-enhanced large language models for perovskite solar cell research | 1 | 2025 |
| OracleKV: Oracle Guidance for Question-Independent KV Cache Compression | 1 | 2025 |
| AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models | 0 | 2025 |
| SSR: Speculative Parallel Scaling Reasoning in Test-time | 0 | 2025 |
| FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management | 0 | 2025 |
| CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring | 0 | 2025 |
| OracleKV: Oracle Guidance for Question-Independent KV Cache Eviction | 0 | N/A |

## Citation History

| Date | Total Citations | H-index |
| ---- | --------------- | ------- |
| 2025-07-21 | 504 | 8 |
| 2025-05-05 | 367 | 7 |
| 2025-04-23 | 357 | 7 |
| 2025-04-11 | 343 | 7 |
| 2025-04-10 | 343 | 7 |
| 2025-04-09 | 343 | 7 |
| 2025-04-08 | 343 | 7 |
| 2025-04-07 | 343 | 7 |
| 2025-04-06 | 343 | 7 |
| 2025-04-05 | 343 | 7 |

## Citation Trends

### Overall Trends
![Citation Trends](citation_trends.png)

### Individual Paper Trends
![Paper Trends](paper_trends.png)

*For interactive charts, see [citation_trends.html](citation_trends.html) and [paper_trends.html](paper_trends.html)*