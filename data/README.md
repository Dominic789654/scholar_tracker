# Citation Statistics Overview

## Latest Statistics
*Last Updated: 2025-07-21*

### Quick Summary
| Metric | Value |
| ------ | ----- |
| Total Citations | 504 |
| H-index | 8 |
| Total Papers | 24 |
| Recent Citation Growth | +137 |

### Today's Changes
- Total Citations Increase: +137
- Papers with new citations:
  - Active prompting with chain-of-thought for large language models: +36 citations
  - LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning: +30 citations
  - Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models: +14 citations
  - LongGenBench: Long-context Generation Benchmark: +14 citations
  - Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models: +7 citations
  - Plum: Prompt learning using metaheuristic: +2 citations
  - Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models: +5 citations
  - ParZC: Parametric Zero-Cost Proxies for Efficient NAS: +4 citations
  - Should We Really Edit Language Models? On the Evaluation of Edited Language Models: +4 citations
  - EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models: +4 citations
  - Can LLMs Maintain Fundamental Abilities under KV Cache Compression?: +3 citations
  - 3D Question Answering for City Scene Understanding: +4 citations
  - The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?: +3 citations
  - Chunkkv: Semantic-preserving kv cache compression for efficient long-context llm inference: +4 citations
  - Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression: +3 citations
  - Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing: +2 citations
  - OracleKV: Oracle Guidance for Question-Independent KV Cache Compression: +1 citations