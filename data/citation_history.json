[
  {
    "date": "2024-11-07",
    "total_citations": 201,
    "h_index": 5,
    "papers": [
      {
        "title": "Active Prompting with Chain-of-Thought for Large Language Models",
        "citations": 156,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric From Scratch for Large Language Models",
        "citations": 11,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning using Metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-07",
    "total_citations": 201,
    "h_index": 5,
    "papers": [
      {
        "title": "Active Prompting with Chain-of-Thought for Large Language Models",
        "citations": 156,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric From Scratch for Large Language Models",
        "citations": 11,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning using Metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-07",
    "total_citations": 201,
    "h_index": 5,
    "papers": [
      {
        "title": "Active Prompting with Chain-of-Thought for Large Language Models",
        "citations": 156,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric From Scratch for Large Language Models",
        "citations": 11,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning using Metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-08",
    "total_citations": 201,
    "h_index": 5,
    "papers": [
      {
        "title": "Active Prompting with Chain-of-Thought for Large Language Models",
        "citations": 156,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric From Scratch for Large Language Models",
        "citations": 11,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning using Metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-08",
    "total_citations": 201,
    "h_index": 5,
    "papers": [
      {
        "title": "Active Prompting with Chain-of-Thought for Large Language Models",
        "citations": 156,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric From Scratch for Large Language Models",
        "citations": 11,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning using Metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-09",
    "total_citations": 201,
    "h_index": 5,
    "papers": [
      {
        "title": "Active Prompting with Chain-of-Thought for Large Language Models",
        "citations": 156,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric From Scratch for Large Language Models",
        "citations": 11,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning using Metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-10",
    "total_citations": 201,
    "h_index": 5,
    "papers": [
      {
        "title": "Active Prompting with Chain-of-Thought for Large Language Models",
        "citations": 156,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric From Scratch for Large Language Models",
        "citations": 11,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning using Metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-11",
    "total_citations": 201,
    "h_index": 5,
    "papers": [
      {
        "title": "Active Prompting with Chain-of-Thought for Large Language Models",
        "citations": 156,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric From Scratch for Large Language Models",
        "citations": 11,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning using Metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-12",
    "total_citations": 202,
    "h_index": 5,
    "papers": [
      {
        "title": "Active Prompting with Chain-of-Thought for Large Language Models",
        "citations": 157,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric From Scratch for Large Language Models",
        "citations": 11,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning using Metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-13",
    "total_citations": 215,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 165,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 16,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 13,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-14",
    "total_citations": 215,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 165,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 16,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 13,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-15",
    "total_citations": 215,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 165,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 16,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 13,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-16",
    "total_citations": 215,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 165,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 16,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 13,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-17",
    "total_citations": 215,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 164,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 13,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-18",
    "total_citations": 216,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 165,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 16,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-19",
    "total_citations": 219,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 166,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-20",
    "total_citations": 219,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 166,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-20",
    "total_citations": 219,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 166,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-21",
    "total_citations": 217,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 165,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-22",
    "total_citations": 217,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 165,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-23",
    "total_citations": 217,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 165,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-24",
    "total_citations": 217,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 165,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-25",
    "total_citations": 218,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 166,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-26",
    "total_citations": 219,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 166,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-27",
    "total_citations": 219,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 166,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-28",
    "total_citations": 221,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 166,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-29",
    "total_citations": 223,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 166,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 18,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-30",
    "total_citations": 223,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 166,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 18,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-01",
    "total_citations": 222,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 166,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 18,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-02",
    "total_citations": 223,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 167,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 18,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-03",
    "total_citations": 223,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 167,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 18,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-04",
    "total_citations": 223,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 167,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 18,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-05",
    "total_citations": 223,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 167,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 18,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-06",
    "total_citations": 224,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 168,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 18,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-07",
    "total_citations": 224,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 168,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 18,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-08",
    "total_citations": 224,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 168,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 18,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-09",
    "total_citations": 224,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 168,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 18,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-10",
    "total_citations": 224,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 168,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 18,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-11",
    "total_citations": 224,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 168,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 18,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-12",
    "total_citations": 224,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 168,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 18,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-13",
    "total_citations": 224,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 168,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 18,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-14",
    "total_citations": 226,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 170,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 18,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-15",
    "total_citations": 228,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 170,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 19,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-16",
    "total_citations": 229,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 170,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 20,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-17",
    "total_citations": 229,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 170,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 20,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-18",
    "total_citations": 229,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 170,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 20,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-19",
    "total_citations": 232,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 171,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 21,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 16,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-20",
    "total_citations": 233,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 172,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 21,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 16,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-21",
    "total_citations": 234,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 172,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 22,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 16,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-22",
    "total_citations": 235,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 172,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 22,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 16,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-23",
    "total_citations": 237,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 173,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 22,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-24",
    "total_citations": 238,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 174,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 22,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-25",
    "total_citations": 238,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 174,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 22,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-26",
    "total_citations": 238,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 174,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 22,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-27",
    "total_citations": 240,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 175,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-28",
    "total_citations": 241,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 176,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-29",
    "total_citations": 242,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 177,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-30",
    "total_citations": 242,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 177,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-31",
    "total_citations": 243,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 177,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-01",
    "total_citations": 243,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 177,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-02",
    "total_citations": 243,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 177,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-03",
    "total_citations": 243,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 177,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-04",
    "total_citations": 243,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 177,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-05",
    "total_citations": 243,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 177,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-06",
    "total_citations": 244,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 178,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-07",
    "total_citations": 244,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 178,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-08",
    "total_citations": 244,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 178,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-09",
    "total_citations": 245,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 178,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 10,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-10",
    "total_citations": 245,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 178,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 10,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-11",
    "total_citations": 246,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 179,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 10,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-12",
    "total_citations": 248,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 181,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 10,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-13",
    "total_citations": 248,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 181,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 10,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-14",
    "total_citations": 249,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 182,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 10,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-15",
    "total_citations": 249,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 182,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 10,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-16",
    "total_citations": 250,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 182,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-17",
    "total_citations": 250,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 182,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-18",
    "total_citations": 250,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 182,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-19",
    "total_citations": 250,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 182,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-20",
    "total_citations": 250,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 182,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-21",
    "total_citations": 250,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 182,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-22",
    "total_citations": 250,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 182,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-23",
    "total_citations": 250,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 182,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-24",
    "total_citations": 251,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 183,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-25",
    "total_citations": 251,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 183,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-26",
    "total_citations": 251,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 183,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-27",
    "total_citations": 251,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 183,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-28",
    "total_citations": 252,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 184,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-29",
    "total_citations": 252,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 184,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-02-02",
    "total_citations": 255,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 184,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 24,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 19,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 10,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-02-03",
    "total_citations": 255,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 184,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 24,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 19,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 10,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-02-03",
    "total_citations": 260,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 188,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 24,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 19,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-02-04",
    "total_citations": 260,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 188,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 24,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 19,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-02-05",
    "total_citations": 260,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 188,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 24,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 19,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-02-06",
    "total_citations": 260,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 188,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 24,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 19,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-02-07",
    "total_citations": 260,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 188,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 24,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 19,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-02-08",
    "total_citations": 263,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 189,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 25,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 20,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-02-09",
    "total_citations": 269,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 191,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 26,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 21,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-10",
    "total_citations": 269,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 191,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 26,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 21,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-11",
    "total_citations": 269,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 191,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 26,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 21,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-12",
    "total_citations": 270,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 191,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 27,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 21,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-13",
    "total_citations": 270,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 191,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 27,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 21,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-14",
    "total_citations": 272,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 191,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 27,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-14",
    "total_citations": 276,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 192,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 28,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 24,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-15",
    "total_citations": 276,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 192,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 28,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 24,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-16",
    "total_citations": 276,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 192,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 28,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 24,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-17",
    "total_citations": 278,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 193,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 29,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 24,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-18",
    "total_citations": 280,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 193,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 31,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 24,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-19",
    "total_citations": 280,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 193,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 31,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 24,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-20",
    "total_citations": 280,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 193,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 31,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 24,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-21",
    "total_citations": 280,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 193,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 31,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 24,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-22",
    "total_citations": 268,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 194,
        "year": "2023"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 26,
        "year": "2024"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-22",
    "total_citations": 287,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 194,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 33,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 26,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-23",
    "total_citations": 291,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 196,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 34,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 26,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-24",
    "total_citations": 291,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 196,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 34,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 26,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-25",
    "total_citations": 295,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 197,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 34,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 29,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-26",
    "total_citations": 295,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 197,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 34,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 29,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-27",
    "total_citations": 296,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 197,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 34,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 30,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-28",
    "total_citations": 299,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 197,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 36,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 30,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-01",
    "total_citations": 300,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 197,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 36,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 30,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-02",
    "total_citations": 300,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 197,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 36,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 30,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-03",
    "total_citations": 303,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 198,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 37,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 31,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-04",
    "total_citations": 304,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 199,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 37,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 31,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-05",
    "total_citations": 308,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 199,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 39,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 32,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-06",
    "total_citations": 308,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 199,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 39,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 32,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-07",
    "total_citations": 309,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 199,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 40,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 32,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-08",
    "total_citations": 310,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 200,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 40,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 32,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-09",
    "total_citations": 310,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 200,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 40,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 32,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  }
]