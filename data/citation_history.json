[
  {
    "date": "2024-11-07",
    "total_citations": 201,
    "h_index": 5,
    "papers": [
      {
        "title": "Active Prompting with Chain-of-Thought for Large Language Models",
        "citations": 156,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric From Scratch for Large Language Models",
        "citations": 11,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning using Metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-07",
    "total_citations": 201,
    "h_index": 5,
    "papers": [
      {
        "title": "Active Prompting with Chain-of-Thought for Large Language Models",
        "citations": 156,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric From Scratch for Large Language Models",
        "citations": 11,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning using Metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-07",
    "total_citations": 201,
    "h_index": 5,
    "papers": [
      {
        "title": "Active Prompting with Chain-of-Thought for Large Language Models",
        "citations": 156,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric From Scratch for Large Language Models",
        "citations": 11,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning using Metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-08",
    "total_citations": 201,
    "h_index": 5,
    "papers": [
      {
        "title": "Active Prompting with Chain-of-Thought for Large Language Models",
        "citations": 156,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric From Scratch for Large Language Models",
        "citations": 11,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning using Metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-08",
    "total_citations": 201,
    "h_index": 5,
    "papers": [
      {
        "title": "Active Prompting with Chain-of-Thought for Large Language Models",
        "citations": 156,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric From Scratch for Large Language Models",
        "citations": 11,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning using Metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-09",
    "total_citations": 201,
    "h_index": 5,
    "papers": [
      {
        "title": "Active Prompting with Chain-of-Thought for Large Language Models",
        "citations": 156,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric From Scratch for Large Language Models",
        "citations": 11,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning using Metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-10",
    "total_citations": 201,
    "h_index": 5,
    "papers": [
      {
        "title": "Active Prompting with Chain-of-Thought for Large Language Models",
        "citations": 156,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric From Scratch for Large Language Models",
        "citations": 11,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning using Metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-11",
    "total_citations": 201,
    "h_index": 5,
    "papers": [
      {
        "title": "Active Prompting with Chain-of-Thought for Large Language Models",
        "citations": 156,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric From Scratch for Large Language Models",
        "citations": 11,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning using Metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-12",
    "total_citations": 202,
    "h_index": 5,
    "papers": [
      {
        "title": "Active Prompting with Chain-of-Thought for Large Language Models",
        "citations": 157,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric From Scratch for Large Language Models",
        "citations": 11,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning using Metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-13",
    "total_citations": 215,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 165,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 16,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 13,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-14",
    "total_citations": 215,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 165,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 16,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 13,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-15",
    "total_citations": 215,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 165,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 16,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 13,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-16",
    "total_citations": 215,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 165,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 16,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 13,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-17",
    "total_citations": 215,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 164,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 13,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-18",
    "total_citations": 216,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 165,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 16,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-19",
    "total_citations": 219,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 166,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-20",
    "total_citations": 219,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 166,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-20",
    "total_citations": 219,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 166,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-21",
    "total_citations": 217,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 165,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-22",
    "total_citations": 217,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 165,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-23",
    "total_citations": 217,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 165,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-24",
    "total_citations": 217,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 165,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-25",
    "total_citations": 218,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 166,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-26",
    "total_citations": 219,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 166,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-27",
    "total_citations": 219,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 166,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-28",
    "total_citations": 221,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 166,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-29",
    "total_citations": 223,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 166,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 18,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-11-30",
    "total_citations": 223,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 166,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 18,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-01",
    "total_citations": 222,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 166,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 18,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-02",
    "total_citations": 223,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 167,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 18,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-03",
    "total_citations": 223,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 167,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 18,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-04",
    "total_citations": 223,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 167,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 18,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-05",
    "total_citations": 223,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 167,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 18,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-06",
    "total_citations": 224,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 168,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 18,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-07",
    "total_citations": 224,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 168,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 18,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-08",
    "total_citations": 224,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 168,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 18,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-09",
    "total_citations": 224,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 168,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 18,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-10",
    "total_citations": 224,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 168,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 18,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-11",
    "total_citations": 224,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 168,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 18,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-12",
    "total_citations": 224,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 168,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 18,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-13",
    "total_citations": 224,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 168,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 18,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-14",
    "total_citations": 226,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 170,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 18,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-15",
    "total_citations": 228,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 170,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 19,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-16",
    "total_citations": 229,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 170,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 20,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-17",
    "total_citations": 229,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 170,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 20,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-18",
    "total_citations": 229,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 170,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 20,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-19",
    "total_citations": 232,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 171,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 21,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 16,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-20",
    "total_citations": 233,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 172,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 21,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 16,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-21",
    "total_citations": 234,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 172,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 22,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 16,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-22",
    "total_citations": 235,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 172,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 22,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 16,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-23",
    "total_citations": 237,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 173,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 22,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-24",
    "total_citations": 238,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 174,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 22,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-25",
    "total_citations": 238,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 174,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 22,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-26",
    "total_citations": 238,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 174,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 22,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-27",
    "total_citations": 240,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 175,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-28",
    "total_citations": 241,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 176,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-29",
    "total_citations": 242,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 177,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-30",
    "total_citations": 242,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 177,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2024-12-31",
    "total_citations": 243,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 177,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-01",
    "total_citations": 243,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 177,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-02",
    "total_citations": 243,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 177,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-03",
    "total_citations": 243,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 177,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-04",
    "total_citations": 243,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 177,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-05",
    "total_citations": 243,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 177,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-06",
    "total_citations": 244,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 178,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-07",
    "total_citations": 244,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 178,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-08",
    "total_citations": 244,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 178,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 9,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-09",
    "total_citations": 245,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 178,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 10,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-10",
    "total_citations": 245,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 178,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 10,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-11",
    "total_citations": 246,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 179,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 10,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-12",
    "total_citations": 248,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 181,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 10,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-13",
    "total_citations": 248,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 181,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 10,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-14",
    "total_citations": 249,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 182,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 10,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-15",
    "total_citations": 249,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 182,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 10,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-16",
    "total_citations": 250,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 182,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-17",
    "total_citations": 250,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 182,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-18",
    "total_citations": 250,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 182,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-19",
    "total_citations": 250,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 182,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-20",
    "total_citations": 250,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 182,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-21",
    "total_citations": 250,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 182,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-22",
    "total_citations": 250,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 182,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-23",
    "total_citations": 250,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 182,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-24",
    "total_citations": 251,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 183,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-25",
    "total_citations": 251,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 183,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-26",
    "total_citations": 251,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 183,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-27",
    "total_citations": 251,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 183,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-28",
    "total_citations": 252,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 184,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-01-29",
    "total_citations": 252,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 184,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-02-02",
    "total_citations": 255,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 184,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 24,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 19,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 10,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-02-03",
    "total_citations": 255,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 184,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 24,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 19,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 10,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-02-03",
    "total_citations": 260,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 188,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 24,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 19,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-02-04",
    "total_citations": 260,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 188,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 24,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 19,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-02-05",
    "total_citations": 260,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 188,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 24,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 19,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-02-06",
    "total_citations": 260,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 188,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 24,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 19,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-02-07",
    "total_citations": 260,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 188,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 24,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 19,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-02-08",
    "total_citations": 263,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 189,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 25,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 20,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 0,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 0,
        "year": "2024"
      }
    ]
  },
  {
    "date": "2025-02-09",
    "total_citations": 269,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 191,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 26,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 21,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-10",
    "total_citations": 269,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 191,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 26,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 21,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-11",
    "total_citations": 269,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 191,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 26,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 21,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-12",
    "total_citations": 270,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 191,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 27,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 21,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-13",
    "total_citations": 270,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 191,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 27,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 21,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-14",
    "total_citations": 272,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 191,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 27,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-14",
    "total_citations": 276,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 192,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 28,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 24,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-15",
    "total_citations": 276,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 192,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 28,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 24,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-16",
    "total_citations": 276,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 192,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 28,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 24,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-17",
    "total_citations": 278,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 193,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 29,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 24,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-18",
    "total_citations": 280,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 193,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 31,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 24,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-19",
    "total_citations": 280,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 193,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 31,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 24,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-20",
    "total_citations": 280,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 193,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 31,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 24,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-21",
    "total_citations": 280,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 193,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 31,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 24,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 11,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-22",
    "total_citations": 268,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 194,
        "year": "2023"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 26,
        "year": "2024"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-22",
    "total_citations": 287,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 194,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 33,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 26,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-23",
    "total_citations": 291,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 196,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 34,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 26,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-24",
    "total_citations": 291,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 196,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 34,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 26,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-25",
    "total_citations": 295,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 197,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 34,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 29,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-26",
    "total_citations": 295,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 197,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 34,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 29,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-27",
    "total_citations": 296,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 197,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 34,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 30,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-02-28",
    "total_citations": 299,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 197,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 36,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 30,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-01",
    "total_citations": 300,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 197,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 36,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 30,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-02",
    "total_citations": 300,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 197,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 36,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 30,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-03",
    "total_citations": 303,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 198,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 37,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 31,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-04",
    "total_citations": 304,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 199,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 37,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 31,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-05",
    "total_citations": 308,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 199,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 39,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 32,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-06",
    "total_citations": 308,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 199,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 39,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 32,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-07",
    "total_citations": 309,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 199,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 40,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 32,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-08",
    "total_citations": 310,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 200,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 40,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 32,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-09",
    "total_citations": 310,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 200,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 40,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 32,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-10",
    "total_citations": 310,
    "h_index": 5,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 200,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 40,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 32,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-12",
    "total_citations": 317,
    "h_index": 6,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 202,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 41,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 33,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-13",
    "total_citations": 322,
    "h_index": 7,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 203,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 41,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 35,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-14",
    "total_citations": 323,
    "h_index": 7,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 203,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 41,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 35,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-15",
    "total_citations": 323,
    "h_index": 7,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 203,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 41,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 35,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-16",
    "total_citations": 323,
    "h_index": 7,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 203,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 41,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 35,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-17",
    "total_citations": 323,
    "h_index": 7,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 203,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 41,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 35,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-18",
    "total_citations": 323,
    "h_index": 7,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 203,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 41,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 35,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-19",
    "total_citations": 325,
    "h_index": 7,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 204,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 42,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 35,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-20",
    "total_citations": 324,
    "h_index": 7,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 203,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 42,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 35,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-21",
    "total_citations": 328,
    "h_index": 7,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 205,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 42,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 36,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-22",
    "total_citations": 328,
    "h_index": 7,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 205,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 42,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 36,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-23",
    "total_citations": 327,
    "h_index": 6,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 205,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 42,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 36,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-24",
    "total_citations": 328,
    "h_index": 7,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 205,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 42,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 36,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-25",
    "total_citations": 328,
    "h_index": 7,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 205,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 42,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 36,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-26",
    "total_citations": 328,
    "h_index": 7,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 205,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 42,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 36,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-27",
    "total_citations": 328,
    "h_index": 7,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 205,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 42,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 36,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-28",
    "total_citations": 331,
    "h_index": 7,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 205,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 42,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 36,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 8,
        "year": "2024"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-29",
    "total_citations": 331,
    "h_index": 7,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 205,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 42,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 36,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 8,
        "year": "2024"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-30",
    "total_citations": 332,
    "h_index": 7,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 205,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 42,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 37,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 8,
        "year": "2024"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-03-31",
    "total_citations": 333,
    "h_index": 7,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 206,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 42,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 37,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 8,
        "year": "2024"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-04-01",
    "total_citations": 334,
    "h_index": 7,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 207,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 42,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 37,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 8,
        "year": "2024"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-04-02",
    "total_citations": 334,
    "h_index": 7,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 207,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 42,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 37,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 8,
        "year": "2024"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-04-03",
    "total_citations": 334,
    "h_index": 7,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 207,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 42,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 37,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 8,
        "year": "2024"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-04-04",
    "total_citations": 336,
    "h_index": 7,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 208,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 42,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 37,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 9,
        "year": "2024"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-04-05",
    "total_citations": 343,
    "h_index": 7,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 211,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 42,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 37,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 9,
        "year": "2024"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-04-06",
    "total_citations": 343,
    "h_index": 7,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 211,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 42,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 37,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 9,
        "year": "2024"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-04-07",
    "total_citations": 343,
    "h_index": 7,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 211,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 42,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 37,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 9,
        "year": "2024"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-04-08",
    "total_citations": 343,
    "h_index": 7,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 211,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 42,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 37,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 9,
        "year": "2024"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-04-09",
    "total_citations": 343,
    "h_index": 7,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 211,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 42,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 37,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 9,
        "year": "2024"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-04-10",
    "total_citations": 343,
    "h_index": 7,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 211,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 42,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 37,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 9,
        "year": "2024"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-04-11",
    "total_citations": 343,
    "h_index": 7,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 211,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 42,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 37,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 9,
        "year": "2024"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-04-23",
    "total_citations": 357,
    "h_index": 7,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 216,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 45,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 41,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 9,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 7,
        "year": "2023"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "Perovskite-llm: Knowledge-enhanced large language models for perovskite solar cell research",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-05-05",
    "total_citations": 367,
    "h_index": 7,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 219,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 46,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 41,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 15,
        "year": "2023"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 10,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 8,
        "year": "2023"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "citations": 3,
        "year": "2025"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "Perovskite-llm: Knowledge-enhanced large language models for perovskite solar cell research",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 1,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-07-21",
    "total_citations": 504,
    "h_index": 8,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 255,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 76,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 55,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 21,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 17,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 11,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 4,
        "year": "2025"
      },
      {
        "title": "Chunkkv: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 4,
        "year": "2025"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 3,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Perovskite-llm: Knowledge-enhanced large language models for perovskite solar cell research",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Eviction",
        "citations": 0,
        "year": "N/A"
      }
    ]
  },
  {
    "date": "2025-07-22",
    "total_citations": 504,
    "h_index": 8,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 255,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 76,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 55,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 21,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 17,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 11,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 4,
        "year": "2025"
      },
      {
        "title": "Chunkkv: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 4,
        "year": "2025"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 3,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Perovskite-llm: Knowledge-enhanced large language models for perovskite solar cell research",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Eviction",
        "citations": 0,
        "year": "N/A"
      }
    ]
  },
  {
    "date": "2025-07-23",
    "total_citations": 505,
    "h_index": 8,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 255,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 77,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 55,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 21,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 17,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 11,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 4,
        "year": "2025"
      },
      {
        "title": "Chunkkv: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 4,
        "year": "2025"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 3,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Perovskite-llm: Knowledge-enhanced large language models for perovskite solar cell research",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Eviction",
        "citations": 0,
        "year": "N/A"
      }
    ]
  },
  {
    "date": "2025-07-24",
    "total_citations": 505,
    "h_index": 8,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 255,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 77,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 55,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 21,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 17,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 13,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 11,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 4,
        "year": "2025"
      },
      {
        "title": "Chunkkv: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 4,
        "year": "2025"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 3,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Perovskite-llm: Knowledge-enhanced large language models for perovskite solar cell research",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Eviction",
        "citations": 0,
        "year": "N/A"
      }
    ]
  },
  {
    "date": "2025-07-26",
    "total_citations": 513,
    "h_index": 8,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 255,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 78,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 55,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 21,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 17,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 14,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 12,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 4,
        "year": "2025"
      },
      {
        "title": "Chunkkv: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 4,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 3,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Perovskite-llm: Knowledge-enhanced large language models for perovskite solar cell research",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-07-25",
    "total_citations": 524,
    "h_index": 9,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 258,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 80,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 55,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 22,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 17,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 15,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 12,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 10,
        "year": "2024"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "Chunkkv: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 4,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 3,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Perovskite-llm: Knowledge-enhanced large language models for perovskite solar cell research",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-07-28",
    "total_citations": 527,
    "h_index": 9,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 259,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 81,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 55,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 17,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 15,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 12,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 10,
        "year": "2024"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "Chunkkv: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 4,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 3,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Perovskite-llm: Knowledge-enhanced large language models for perovskite solar cell research",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-07-29",
    "total_citations": 529,
    "h_index": 9,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 259,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 81,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 55,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 24,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 17,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 17,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 16,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 12,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 10,
        "year": "2024"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "Chunkkv: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 4,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 3,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Perovskite-llm: Knowledge-enhanced large language models for perovskite solar cell research",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-08-02",
    "total_citations": 531,
    "h_index": 9,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 259,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 81,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 56,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 24,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 18,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 17,
        "year": "2023"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 16,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 12,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 10,
        "year": "2024"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "Chunkkv: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 4,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 3,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Perovskite-llm: Knowledge-enhanced large language models for perovskite solar cell research",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-08-09",
    "total_citations": 537,
    "h_index": 9,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 262,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 82,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 56,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 24,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 18,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 18,
        "year": "2023"
      },
      {
        "title": "Plum: Prompt learning using metaheuristic",
        "citations": 17,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 12,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 10,
        "year": "2024"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "Chunkkv: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 4,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 3,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Perovskite-llm: Knowledge-enhanced large language models for perovskite solar cell research",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-08-21",
    "total_citations": 566,
    "h_index": 9,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 273,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 84,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 56,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning Using Metaheuristics",
        "citations": 29,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 25,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 18,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 18,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 12,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 10,
        "year": "2024"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 3,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Perovskite-llm: Knowledge-enhanced large language models for perovskite solar cell research",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-08-28",
    "total_citations": 577,
    "h_index": 9,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 278,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 86,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 57,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning Using Metaheuristics",
        "citations": 29,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 24,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 20,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 18,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 10,
        "year": "2024"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 5,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 3,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Perovskite-llm: Knowledge-enhanced large language models for perovskite solar cell research",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-08-31",
    "total_citations": 581,
    "h_index": 9,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 279,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 86,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 57,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning Using Metaheuristics",
        "citations": 29,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 25,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 20,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 18,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 10,
        "year": "2024"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 3,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Perovskite-llm: Knowledge-enhanced large language models for perovskite solar cell research",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-09-03",
    "total_citations": 581,
    "h_index": 9,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 279,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 86,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 57,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning Using Metaheuristics",
        "citations": 29,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 25,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 20,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 18,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 10,
        "year": "2024"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 3,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Perovskite-llm: Knowledge-enhanced large language models for perovskite solar cell research",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-09-04",
    "total_citations": 581,
    "h_index": 9,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 279,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 86,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 57,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning Using Metaheuristics",
        "citations": 29,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 25,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 20,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 18,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 10,
        "year": "2024"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 3,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Perovskite-llm: Knowledge-enhanced large language models for perovskite solar cell research",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-09-09",
    "total_citations": 584,
    "h_index": 9,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 281,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 86,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 57,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning Using Metaheuristics",
        "citations": 29,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 26,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 20,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 18,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 10,
        "year": "2024"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 3,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-09-15",
    "total_citations": 591,
    "h_index": 9,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 284,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 87,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 58,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning Using Metaheuristics",
        "citations": 29,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 28,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 20,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 18,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 10,
        "year": "2024"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 3,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-09-18",
    "total_citations": 593,
    "h_index": 9,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 285,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 88,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 58,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning Using Metaheuristics",
        "citations": 29,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 28,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 20,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 18,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 10,
        "year": "2024"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 3,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-09-20",
    "total_citations": 595,
    "h_index": 9,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 285,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 88,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 58,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning Using Metaheuristics",
        "citations": 29,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 28,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 21,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 18,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 10,
        "year": "2024"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 3,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-09-22",
    "total_citations": 598,
    "h_index": 9,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 287,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 88,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 58,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning Using Metaheuristics",
        "citations": 29,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 28,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 21,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 18,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 14,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 10,
        "year": "2024"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 4,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-09-27",
    "total_citations": 608,
    "h_index": 9,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 291,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 91,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 58,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning Using Metaheuristics",
        "citations": 31,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 28,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 21,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 18,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 10,
        "year": "2024"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 4,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-09-28",
    "total_citations": 607,
    "h_index": 9,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 291,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 90,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 58,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning Using Metaheuristics",
        "citations": 31,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 28,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 21,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 18,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 10,
        "year": "2024"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 4,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-09-29",
    "total_citations": 607,
    "h_index": 9,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 291,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 90,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 58,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning Using Metaheuristics",
        "citations": 31,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 28,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 21,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 18,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 10,
        "year": "2024"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 4,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-10-05",
    "total_citations": 618,
    "h_index": 9,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 295,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 93,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 59,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning Using Metaheuristics",
        "citations": 31,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 28,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 18,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 10,
        "year": "2024"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 4,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-10-06",
    "total_citations": 618,
    "h_index": 9,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 295,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 93,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 59,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning Using Metaheuristics",
        "citations": 31,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 28,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 18,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 10,
        "year": "2024"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 4,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-10-09",
    "total_citations": 620,
    "h_index": 9,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 296,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 93,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 59,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning Using Metaheuristics",
        "citations": 31,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 28,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 18,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 10,
        "year": "2024"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 4,
        "year": "2025"
      },
      {
        "title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 2,
        "year": "2024"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-10-10",
    "total_citations": 626,
    "h_index": 9,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 298,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 94,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 59,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning Using Metaheuristics",
        "citations": 31,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 29,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 18,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 10,
        "year": "2024"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 4,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-10-14",
    "total_citations": 629,
    "h_index": 9,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 300,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 94,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 60,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning Using Metaheuristics",
        "citations": 31,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 29,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 18,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 10,
        "year": "2024"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 4,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-10-15",
    "total_citations": 630,
    "h_index": 9,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 300,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 95,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 60,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning Using Metaheuristics",
        "citations": 31,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 29,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 18,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 10,
        "year": "2024"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 4,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-10-16",
    "total_citations": 629,
    "h_index": 9,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 299,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 95,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 60,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning Using Metaheuristics",
        "citations": 31,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 29,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 18,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 10,
        "year": "2024"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 4,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-10-18",
    "total_citations": 633,
    "h_index": 9,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 300,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 96,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 62,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning Using Metaheuristics",
        "citations": 31,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 29,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 18,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 10,
        "year": "2024"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 4,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-10-19",
    "total_citations": 633,
    "h_index": 9,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 300,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 96,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 62,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning Using Metaheuristics",
        "citations": 31,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 29,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 18,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 10,
        "year": "2024"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 4,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-10-20",
    "total_citations": 637,
    "h_index": 9,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 304,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 96,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 62,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning Using Metaheuristics",
        "citations": 31,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 29,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 18,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 10,
        "year": "2024"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 4,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 3,
        "year": "2024"
      },
      {
        "title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-10-22",
    "total_citations": 642,
    "h_index": 9,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 304,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 97,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 62,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning Using Metaheuristics",
        "citations": 31,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 29,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 18,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 10,
        "year": "2024"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-10-23",
    "total_citations": 643,
    "h_index": 9,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 304,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 98,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 62,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning Using Metaheuristics",
        "citations": 31,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 29,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 23,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 18,
        "year": "2023"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 15,
        "year": "2024"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 10,
        "year": "2024"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-10-25",
    "total_citations": 664,
    "h_index": 9,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 305,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 106,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 66,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning Using Metaheuristics",
        "citations": 33,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 29,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 27,
        "year": "2024"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 19,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 18,
        "year": "2023"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 11,
        "year": "2024"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 4,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "Reasoning Language Model Inference Serving Unveiled: An Empirical Study",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-10-27",
    "total_citations": 670,
    "h_index": 9,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 306,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 106,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 66,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning Using Metaheuristics",
        "citations": 34,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 30,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 28,
        "year": "2024"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 19,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 18,
        "year": "2023"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 11,
        "year": "2024"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 8,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 4,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "Reasoning Language Model Inference Serving Unveiled: An Empirical Study",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-10-28",
    "total_citations": 674,
    "h_index": 9,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 309,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 107,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 66,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning Using Metaheuristics",
        "citations": 34,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 30,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 28,
        "year": "2024"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 19,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 18,
        "year": "2023"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 11,
        "year": "2024"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 8,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 4,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "DiffAdapt: Difficulty-Adaptive Reasoning for Token-Efficient LLM Inference",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Reasoning Language Model Inference Serving Unveiled: An Empirical Study",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-11-01",
    "total_citations": 679,
    "h_index": 9,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 311,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 108,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 67,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning Using Metaheuristics",
        "citations": 34,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 30,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 29,
        "year": "2024"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 19,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 18,
        "year": "2023"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 11,
        "year": "2024"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 8,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 4,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "DiffAdapt: Difficulty-Adaptive Reasoning for Token-Efficient LLM Inference",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Reasoning Language Model Inference Serving Unveiled: An Empirical Study",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-11-03",
    "total_citations": 683,
    "h_index": 9,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 311,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 108,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 67,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning Using Metaheuristics",
        "citations": 34,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 30,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 29,
        "year": "2024"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 19,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 18,
        "year": "2023"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 11,
        "year": "2024"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 8,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 8,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 7,
        "year": "2025"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "DiffAdapt: Difficulty-Adaptive Reasoning for Token-Efficient LLM Inference",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Reasoning Language Model Inference Serving Unveiled: An Empirical Study",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "An Improved Autoregressive Evaluation Paradigm for Large Language Models",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-11-05",
    "total_citations": 695,
    "h_index": 9,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 315,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 109,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 68,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning Using Metaheuristics",
        "citations": 34,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 32,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 30,
        "year": "2024"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 19,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 19,
        "year": "2023"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 12,
        "year": "2024"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 8,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 8,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 8,
        "year": "2025"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "DiffAdapt: Difficulty-Adaptive Reasoning for Token-Efficient LLM Inference",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Reasoning Language Model Inference Serving Unveiled: An Empirical Study",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "An Improved Autoregressive Evaluation Paradigm for Large Language Models",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-11-07",
    "total_citations": 707,
    "h_index": 9,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 318,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 112,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 69,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning Using Metaheuristics",
        "citations": 34,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 32,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 32,
        "year": "2024"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 20,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 19,
        "year": "2023"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 12,
        "year": "2024"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 9,
        "year": "2025"
      },
      {
        "title": "ChunkKV: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 9,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 8,
        "year": "2025"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 6,
        "year": "2024"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "DiffAdapt: Difficulty-Adaptive Reasoning for Token-Efficient LLM Inference",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Reasoning Language Model Inference Serving Unveiled: An Empirical Study",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "An Improved Autoregressive Evaluation Paradigm for Large Language Models",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-11-13",
    "total_citations": 714,
    "h_index": 10,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 317,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 116,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 69,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning Using Metaheuristics",
        "citations": 34,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 33,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 33,
        "year": "2024"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 20,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 19,
        "year": "2023"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 12,
        "year": "2024"
      },
      {
        "title": "ChunkKV: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 10,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 9,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 8,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "Beyond Single Embeddings: Capturing Diverse Targets with Multi-Query Retrieval",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "DiffAdapt: Difficulty-Adaptive Reasoning for Token-Efficient LLM Inference",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Reasoning Language Model Inference Serving Unveiled: An Empirical Study",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "An Improved Autoregressive Evaluation Paradigm for Large Language Models",
        "citations": 0,
        "year": "2025"
      }
    ]
  },
  {
    "date": "2025-11-14",
    "total_citations": 716,
    "h_index": 10,
    "papers": [
      {
        "title": "Active prompting with chain-of-thought for large language models",
        "citations": 318,
        "year": "2023"
      },
      {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "citations": 117,
        "year": "2024"
      },
      {
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
        "citations": 69,
        "year": "2024"
      },
      {
        "title": "Plum: Prompt Learning Using Metaheuristics",
        "citations": 34,
        "year": "2024"
      },
      {
        "title": "LongGenBench: Long-context Generation Benchmark",
        "citations": 33,
        "year": "2024"
      },
      {
        "title": "Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models",
        "citations": 33,
        "year": "2024"
      },
      {
        "title": "ParZC: Parametric Zero-Cost Proxies for Efficient NAS",
        "citations": 20,
        "year": "2024"
      },
      {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",
        "citations": 19,
        "year": "2023"
      },
      {
        "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
        "citations": 12,
        "year": "2024"
      },
      {
        "title": "ChunkKV: Semantic-preserving kv cache compression for efficient long-context llm inference",
        "citations": 10,
        "year": "2025"
      },
      {
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "citations": 9,
        "year": "2025"
      },
      {
        "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
        "citations": 8,
        "year": "2025"
      },
      {
        "title": "3D Question Answering for City Scene Understanding",
        "citations": 7,
        "year": "2024"
      },
      {
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "citations": 6,
        "year": "2025"
      },
      {
        "title": "Mediator: Memory-efficient llm merging with less parameter conflicts and uncertainty based routing",
        "citations": 5,
        "year": "2025"
      },
      {
        "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
        "citations": 4,
        "year": "2024"
      },
      {
        "title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "OracleKV: Oracle Guidance for Question-Independent KV Cache Compression",
        "citations": 2,
        "year": "2025"
      },
      {
        "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "citations": 1,
        "year": "2025"
      },
      {
        "title": "Beyond Single Embeddings: Capturing Diverse Targets with Multi-Query Retrieval",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "DiffAdapt: Difficulty-Adaptive Reasoning for Token-Efficient LLM Inference",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "Reasoning Language Model Inference Serving Unveiled: An Empirical Study",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
        "citations": 0,
        "year": "2025"
      },
      {
        "title": "An Improved Autoregressive Evaluation Paradigm for Large Language Models",
        "citations": 0,
        "year": "2025"
      }
    ]
  }
]